{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bce434-57ad-4b35-91c4-a20b16c8a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn\n",
    "!pip install pymorphy3\n",
    "!pip install -U pymorphy3-dicts-ru\n",
    "!pip install umap-learn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6735063-4298-4baf-a56c-66c8ec399078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pymorphy3\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c9fb3-9be8-4764-8f13-34bd78f19f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENSORED_MODE = True # установите False, чтобы снять защиту цензуры\n",
    "\n",
    "BASE_COLUMN = 'text' if CENSORED_MODE else 'censored'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed118a81-da57-4320-8e2d-24234c6a353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('processed')\n",
    "\n",
    "stop_words = []\n",
    "with open('./stop_words.txt') as f:\n",
    "    stop_words = [item.replace('\\n', '') for item in f.readlines()]\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fda7bb-24ba-4297-ab01-97be318a6427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция чтения .json файла и преобразования в dict\n",
    "def get_file_as_dict(name: str) -> dict:\n",
    "    with open(f'./dataset_raw/{name}', 'r') as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "dataset = [get_file_as_dict(name) for name in files if '.json' in name ] # преобразуем данные в \n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13dd1a0-4160-4aac-ad70-0c4728e445fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][BASE_COLUMN][:100]) # первые 100 символов из текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f7342-8eae-48f1-b138-f21afb72f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "symbols_to_drop = set(('(', ')', '.', ',', '?', '!', '«', '»', '—', '-', '’', '…', '(', ')', '’', '–', '\"'))\n",
    "\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "for item in dataset:\n",
    "    item['original'] = item['text']\n",
    "    item['original_censored'] = item['censored']\n",
    "    # Переводим все в lowercase и удаляем лишние пробелы\n",
    "    item['title'] = item['title'].lower().strip()\n",
    "    item['text'] = item['text'].lower().strip()\n",
    "    item['censored'] = item['censored'].lower().strip()\n",
    "\n",
    "    # уничтожаем спец символы в результате парсинга\n",
    "    item['text'] = item['text'].replace('\\xa0', ' ')\n",
    "    item['censored'] = item['censored'].replace('\\xa0', ' ')\n",
    "\n",
    "    item['text'] = item['text'].replace('\\u2005', ' ')\n",
    "    item['censored'] = item['censored'].replace('\\u2005', ' ')\n",
    "\n",
    "    item['text'] = item['text'].replace('\\n', ' ')\n",
    "    item['censored'] = item['censored'].replace('\\n', ' ')\n",
    "\n",
    "    # удаляем ненужные символы\n",
    "    for symbol in symbols_to_drop:\n",
    "        item['text'] = item['text'].replace(symbol, ' ')\n",
    "        item['censored'] = item['censored'].replace(symbol, ' ')\n",
    "\n",
    "    item['text'] = remove_extra_spaces(item['text'])\n",
    "    item['censored'] = remove_extra_spaces(item['censored'])\n",
    "    \n",
    "    # удаляем стоп слова\n",
    "    item['text'] = ' '.join([word for word in item['text'].split(' ') if word not in stop_words])\n",
    "    item['censored'] = ' '.join([word for word in item['censored'].split(' ') if word not in stop_words])\n",
    "    \n",
    "dataset[0]['censored'][:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906a851-b8aa-4877-9ec8-c79dc52c7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "morph = pymorphy3.MorphAnalyzer() # морфологический анализатор текста, если у вас не работает, тогда pymorphy3 необходимо заменить на pymorphy2 (в install, в импортах и так далее. процесс установки итп есть в документации)\n",
    "\n",
    "pprint(morph.parse('был')[0].normal_form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac3659-be39-4b24-b0bb-11ab8e788278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем все (леммы, слова, тоекены - это одно и тоже) в нормальную форму (начальная форма, единственное число для каждого типа слова)\n",
    "\n",
    "for item in dataset:\n",
    "    item['text_normalized'] = ' '.join([morph.parse(word)[0].normal_form for word in item['text'].split(' ')])\n",
    "    item['censored_normalized'] = ' '.join([morph.parse(word)[0].normal_form for word in item['censored'].split(' ') if word not in stop_words or len(word) < 2])\n",
    "\n",
    "dataset[0]['censored_normalized'][:100] # пример НФ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d512eb-c45c-41e8-b4e8-a2e81fee5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENSORED_COLUMNS = ['text', 'text_normalized'] if not CENSORED_MODE else []\n",
    "\n",
    "df = pd.DataFrame.from_dict(dataset)\n",
    "columns = df.columns\n",
    "censoder_mode_columns = ['title', 'censored', 'censored_normalized'] + CENSORED_COLUMNS\n",
    "\n",
    "df[censoder_mode_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d179f-d88b-4313-91af-7a17d4713105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "corpus = df.text_normalized.to_list()\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "hashing_vectorizer = HashingVectorizer(n_features=1000)\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2ebd0-ca53-4be6-ac42-75caa51c5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count_vectorizer = count_vectorizer.fit_transform(corpus)\n",
    "X_hashing_vectorizer = hashing_vectorizer.fit_transform(corpus)\n",
    "X_tfidf_vectorizer = tfidf_vectorizer.fit_transform(corpus)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1b7d8-3361-4dab-95e3-06274e48cab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_count_vectorizer.toarray().shape)\n",
    "list(X_count_vectorizer.toarray())[:1][0][:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f19e3-023e-40c3-9611-25e7d2d0c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_hashing_vectorizer.toarray().shape)\n",
    "list(X_hashing_vectorizer.toarray())[:1][0][:600] # это короче вектор из всех слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d1d3c-7742-43e6-a14d-ccca14525f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_tfidf_vectorizer.toarray().shape)\n",
    "list(X_tfidf_vectorizer.toarray())[:1][0][:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945fda65-054a-463f-b8e6-98a929d6c54d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = UMAP(n_components=10, n_neighbors=4, random_state=42, n_jobs=1).fit_transform(X_tfidf_vectorizer) # для дальнейшей обработки\n",
    "X_embedded_visual = UMAP(n_components = 2, n_neighbors=4, random_state=42, n_jobs=1).fit_transform(X_tfidf_vectorizer) # для визуализации\n",
    "X_embedded[:5] # преобразуем наши вектора с длиной 991 элементов, в вектор размером в 20 элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032a9e3-35d8-4ed1-a000-77267fa1f5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_embedded_visual[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b313ed73-b136-43d8-a7d9-d395f109aaf2",
   "metadata": {},
   "source": [
    "Попробуем визуализировать и увидеть следующее:\n",
    "\n",
    "1. группы точек, которые близки друг к другу, указывают на то, что песни в имеют схожую тематику, которая выражается похожим наборов слов\n",
    "\n",
    "2. Точки, которые далеко от всех остальных,представляют песни, которые уникальны по своему содержанию или стилю (в нашем случае вероятно по языку)\n",
    "\n",
    "3. Области с большей плотностью точек могут указывать на наиболее общие или популярные слова в корпусе песен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fc96d-3018-4e15-a814-cff3dab705c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_embedded_visual[:, 0], y=X_embedded_visual[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032fb43d-3121-4531-b58c-ff7938c7c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация списка для хранения оценок силуэта\n",
    "kmeans_scores = []\n",
    "\n",
    "# Перебор различных значений k (от 2 до 10)\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_tfidf_vectorizer)\n",
    "    score = silhouette_score(X_tfidf_vectorizer, kmeans.labels_)\n",
    "    kmeans_scores.append(score)\n",
    "\n",
    "print(kmeans_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686247c4-610e-4d66-b9f8-71f69a4687a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "fig, axes = plt.subplots(7, 2, figsize=(12, 30))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Удаляем последний график, так как он не нужен в данной конфигурации\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "scores = []\n",
    "for i, k in enumerate(range(2, 15)):\n",
    "    # Обучение KMeans модели\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_embedded)\n",
    "    \n",
    "    # Рассчет silhouette score\n",
    "    score = silhouette_score(X_embedded, kmeans.labels_)\n",
    "    scores.append(score)\n",
    "    # Визуализация\n",
    "    cluster_centers_visual = np.array([X_embedded_visual[kmeans.labels_ == label].mean(axis=0) for label in np.unique(kmeans.labels_)])\n",
    "    \n",
    "    axes[i].scatter(X_embedded_visual[:, 0], X_embedded_visual[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "    axes[i].scatter(cluster_centers_visual[:, 0], cluster_centers_visual[:, 1], c='red', marker='X')\n",
    "\n",
    "    for center in cluster_centers_visual:\n",
    "        circle = Circle(center, radius=0.2, fill=False, linestyle='-', linewidth=2, edgecolor='red')\n",
    "        axes[i].add_artist(circle)\n",
    "    \n",
    "    axes[i].set_title(f'K = {k}, Silhouette Score = {score:.2f}')\n",
    "\n",
    "print(scores)\n",
    "\n",
    "optimal_cluster_k = scores.index(max(scores)) + 2\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d95912-66e6-40ea-8d95-801706e9065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "kmeans = KMeans(n_clusters=7, n_init=10, random_state=42)\n",
    "kmeans.fit(X_embedded_visual)\n",
    "kmeans_df = df.copy()\n",
    "kmeans_df['cluster_label'] = kmeans.labels_\n",
    "\n",
    "Counter(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b0752-582b-4c04-b691-bfd04a06b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df['censored_normalized'] # текст песни\n",
    "kmeans_df['cluster_label'] # лейбл кластера\n",
    "unique_clusters = sorted(kmeans_df['cluster_label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8f917-0258-429a-8227-d92bbd06b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Итерируемся по уникальным кластерам\n",
    "cluster_text_top5_counters = {}\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    # Фильтруем строки по текущему кластеру и объединяем тексты в один большой список\n",
    "    cluster_texts = kmeans_df[kmeans_df['cluster_label'] == cluster][f'{BASE_COLUMN}_normalized'].tolist()\n",
    "    concatenated_texts = ' '.join(cluster_texts)\n",
    "\n",
    "    # Считаем частоту каждого слова в данном кластере и отбираем топ 10\n",
    "    counter = Counter(concatenated_texts.split())\n",
    "    top_5_words = counter.most_common(5)\n",
    "\n",
    "    cluster_text_top5_counters[cluster] = top_5_words\n",
    "\n",
    "# Выводим результаты (в данном случае, они будут простыми из-за фиктивных данных)\n",
    "cluster_text_top5_counters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befaf6da-84d0-4df4-8256-32df27fe5d89",
   "metadata": {},
   "source": [
    "Как мы можем интерпретировать? ->\n",
    "\n",
    "Кластер 0: Содержит слова, которые можно ассоциировать с состоянием неопределенности или волнения (\"стоить\", \"бояться\", \"черта\", \"ждать\"). Есть также слова, указывающие на временную перспективу (\"навсегда\") и действия (\"удалить\").\n",
    "\n",
    "Кластер 5: Содержит некоторые негативно окрашенные слова (\"цензура\", \"страдать\", \"дрянный\") и слова, связанные с планетарными или космическими темами (\"планета\").\n",
    "\n",
    "Кластер 1: Смешивает английский и русский языки и содержит слова с сильной эмоциональной окраской (\"цензура\", \"цензура\", \"you\", \"me\").\n",
    "\n",
    "Кластер 4: Включает слова, связанные с отношениями и эмоциями (\"дать\", \"уйти\", \"любить\", \"ангел\").\n",
    "\n",
    "Кластер 3: Содержит слова, ассоциирующиеся с физиологией и эмоциональными состояниями (\"любовь\", \"гормон\", \"кровь\").\n",
    "\n",
    "Кластер 6: Смешивает немецкий и русский языки и включает слова, связанные с желаниями и страхами (\"хотеть\", \"бояться\").\n",
    "\n",
    "Кластер 2: Содержит слова, связанные с негативными эмоциями и действиями (\"врать\", \"плакать\", \"бросить\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb30f11-5685-471e-a926-b256cbbd9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df[kmeans_df.cluster_label == 0].title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee619ed-6d72-488f-9502-7cb8f95538cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Создаем пустой список, в который будем добавлять средние расстояния\n",
    "neighbours = []\n",
    "\n",
    "# Проходимся по каждой строке в исходном DataFrame\n",
    "for i, v1 in enumerate(X_embedded):\n",
    "    # Вычисляем расстояния от текущей точки до всех остальных\n",
    "    distances = [np.linalg.norm(v1 - v2) for j, v2 in enumerate(X_embedded) if i != j]\n",
    "    # Вычисляем среднее расстояние до 5 ближайших соседей\n",
    "    neighbours.append(np.mean(sorted(distances)[:5]))\n",
    "\n",
    "# Сортируем средние расстояния\n",
    "neighbours = sorted(neighbours)\n",
    "\n",
    "sns.lineplot(x=range(len(neighbours)), y=neighbours)\n",
    "plt.axhline(y=np.mean(neighbours) + np.std(neighbours) / 2, color='g', linestyle='--', label=\"Mean + 0.5*STD\")\n",
    "plt.axhline(y=np.mean(neighbours), color='r', linestyle='--', label=\"Mean\")\n",
    "plt.axhline(y=np.mean(neighbours) - np.std(neighbours) / 2, color='g', linestyle='--', label=\"Mean - 0.5*STD\")\n",
    "plt.legend()\n",
    "plt.title(\"Sorted Mean Distances to 5 Nearest Neighbours\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Mean Distance\")\n",
    "plt.show()\n",
    "\n",
    "optimal_eps = np.mean(neighbours)\n",
    "optimal_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271ec01-68df-4bc6-942e-734d2c8a290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_metrics = [np.mean(neighbours) - np.std(neighbours) / 2, np.mean(neighbours), np.mean(neighbours) + np.std(neighbours) / 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ffa67-fc19-40df-ae2c-61c5439a3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_scores = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, eps in enumerate(eps_metrics):\n",
    "    # Обучение DBSCAN модели\n",
    "    dbsscan = DBSCAN(eps=eps, min_samples=2)\n",
    "    dbsscan.fit(X_embedded)\n",
    "\n",
    "    # Получение меток кластеров\n",
    "    labels = dbsscan.labels_\n",
    "\n",
    "    # Исключение шумовых точек для расчета центроида\n",
    "    non_noise_indices = np.where(labels != -1)\n",
    "    non_noise_labels = labels[non_noise_indices]\n",
    "    non_noise_X = X_embedded[non_noise_indices]\n",
    "\n",
    "    # Расчет центроидов для каждого кластера\n",
    "    unique_labels = np.unique(non_noise_labels)\n",
    "    cluster_centers = np.array([non_noise_X[non_noise_labels == label].mean(axis=0) for label in unique_labels])\n",
    "\n",
    "    # Расчет Davies-Bouldin Index\n",
    "    if len(unique_labels) > 1:  # DB index требует хотя бы два кластера\n",
    "        db_score = davies_bouldin_score(non_noise_X, non_noise_labels)\n",
    "        db_scores.append(db_score)\n",
    "    else:\n",
    "        db_scores.append(None)\n",
    "\n",
    "    # Визуализация\n",
    "    axes[i].scatter(X_embedded[:, 0], X_embedded[:, 1], c=labels, cmap='viridis')\n",
    "    if len(cluster_centers) > 0:  # Проверка на случай, если нет кластеров\n",
    "        axes[i].scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='x')\n",
    "    axes[i].set_title(f'eps = {round(eps, 4)}, Davies-Bouldin Score = {round(db_score, 4) if db_score is not None else \"N/A\"}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5546821-f3f5-4ffe-bf99-faf7515b11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = np.mean(neighbours) - np.std(neighbours) / 2\n",
    "\n",
    "dbsscan = DBSCAN(eps=best_metric, min_samples=2)\n",
    "dbsscan.fit(X_embedded)\n",
    "    \n",
    " # Получение меток кластеров\n",
    "labels = dbsscan.labels_\n",
    "dbscan_df = df.copy()\n",
    "dbscan_df['cluster_label'] = dbsscan.labels_\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa0512-ad9b-457a-b82a-e5de22d39e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_clusters = sorted(dbscan_df['cluster_label'].unique())\n",
    "\n",
    "cluster_text_top5_counters = {}\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    # Фильтруем строки по текущему кластеру и объединяем тексты в один большой список\n",
    "    cluster_texts = dbscan_df[dbscan_df['cluster_label'] == cluster][f'{BASE_COLUMN}_normalized'].tolist()\n",
    "    concatenated_texts = ' '.join(cluster_texts)\n",
    "\n",
    "    # Считаем частоту каждого слова в данном кластере и отбираем топ 10\n",
    "    counter = Counter(concatenated_texts.split())\n",
    "    top_5_words = counter.most_common(5)\n",
    "    \n",
    "    cluster_text_top5_counters[cluster] = top_5_words\n",
    "\n",
    "# Выводим результаты (в данном случае, они будут простыми из-за фиктивных данных)\n",
    "cluster_text_top5_counters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1637e7dc-c574-4c90-bdf2-0802e31ffbdc",
   "metadata": {},
   "source": [
    "Интерпретация кластеров\n",
    "\n",
    "Кластер 0: Слова в этом кластере, такие как \"хотеть\", \"бояться\", \"стоить\", \"знать\", и некоторые немецкие слова, могут указывать на темы связанные с желаниями, страхами и знанием. Это может быть кластер, включающий слова, которые часто употребляются в контексте принятия решений или эмоциональных переживаний.\n",
    "\n",
    "Кластер 1: Слова как \"ести\", \"заваль\", \"нужный\", \"cash\" предположительно связаны с потребностями и материальными аспектами.\n",
    "\n",
    "Кластер 2: Слова на английском языке (\"цензура\", \"цензура\", \"цензура\") и русские нецензурные выражения (\"цензура\", \"цензура\") предположительно относятся к более жёсткому, возможно, конфликтному общению.\n",
    "\n",
    "Кластер 3: Слова как \"азиатка\", \"равно\", \"эгоэгоэгоист\" могут указывать на какие-то культурные или психологические аспекты.\n",
    "\n",
    "Кластер 4: Этот кластер может быть связан с женским полом или эмоциями (\"девочка\", \"скакать\", \"мой\").\n",
    "\n",
    "Кластер 5: Слова вроде \"дать\", \"уйти\", \"любить\" могут быть связаны с межличностными отношениями.\n",
    "\n",
    "Кластер 6: Слова как \"гормон\", \"предвестник\", \"кровь\" могут указывать на физиологические или медицинские аспекты.\n",
    "\n",
    "Кластер 7: Этот кластер может быть связан с эмоциональными и физическими аспектами (\"мой\", \"рука\", \"амбиция\").\n",
    "\n",
    "Кластер 8: Слова вроде \"врать\", \"плакать\", \"цензура\" могут указывать на отрицательные эмоции или конфликтные ситуации.\n",
    "\n",
    "Кластер 9: Здесь много нецензурных или грубых слов, что может указывать на агрессивное или негативное общение.\n",
    "\n",
    "Кластер 10: Слова как \"цензура\", \"самый\", \"страдать\" также предполагают негативные эмоции или отношения.\n",
    "\n",
    "Кластер 11: Слова вроде \"убить\", \"сердце\", \"шрам\" могут быть связаны с темами потери, страдания или эмоционального воздействия.\n",
    "\n",
    "Кластер 12: Этот кластер может иметь отношение к религиозным или духовным аспектам (\"ангел\").\n",
    "\n",
    "-1: этот кластер содержит слова, которые не удалось чётко отнести к другим кластерам. Слова как \"остановиться\", \"танцевать\", \"рассвет\" могут указывать на разнообразные темы от эмоций до действий. (в контексте DBSCAN - кластер -1 - является тем кластером, куда ничто не вошло)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9a951-d5e9-4ea0-af96-8c351c2d7dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
